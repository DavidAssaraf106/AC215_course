{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"exercise_4.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"MfTh4bUzAmSX"},"source":["<h1 style=\"padding-top: 25px;padding-bottom: 25px;text-align: left; padding-left: 10px; background-color: #DDDDDD; \n","    color: black;\"> <img style=\"float: left; padding-right: 10px;\" src=\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/iacs.png\" height=\"50px\"> <a href='https://harvard-iacs.github.io/2021-AC215/' target='_blank'><strong><font color=\"#A41034\">AC215: Advanced Practical Data Science, MLOps</font></strong></a></h1>\n","\n","# **<font color=\"#A41034\">Exercise 4 - Language Models</font>**\n","\n","**Harvard University**<br/>\n","**Fall 2021**<br/>\n","**Instructor:**<br/>\n","Pavlos Protopapas\n","\n","<hr style=\"height:2pt\">"]},{"cell_type":"markdown","metadata":{"id":"pWSXnfQUStq5"},"source":["## **<font color=\"#A41034\">Competition</font>**\n","\n","### **<font color=\"#f03b20\">Due Date: Check Canvas</font>**\n","\n","#### **[Join Competition](https://www.kaggle.com/t/a8ec65d928b645e596a071527b3b3d33)**\n","\n","#### **[View Leaderboard](https://www.kaggle.com/c/ac215-fall-2021/leaderboard)**\n","\n","Now your task for this exercise is to build the best language model capable of classifying **abstracts** of Astrophysics papers taken from ArXiv. The labels for the classification dataset are as following: \n","\n","**Lables:**\n","\n","* 0 = astro-ph.SR - Solar and Stellar Astrophysics\n","* 1 = astro-ph.GA - Astrophysics of Galaxies\n","* 2 = astro-ph.CO - Cosmology and Nongalactic Astrophysics\n","\n","A good start for this exercise would be to start with some pre-trained language models. Since the text in Astrophysics is very domain specific, there is only so much accuracy a pre-trained language model can acheive. An idea to build a better language model will be to finetune the language model on a lot of unlabled abstracts from the Astrophysics papers. Then use this finetuned model for classification.\n","\n","Here are some techniques you can try:\n","\n","* Transfer Learning using different pre-trained language models from [HuggingFace](https://huggingface.co/models)\n","* Finetune a language model using the the abstract texts provided (`abstracts_train.txt` & `datasets/abstracts_validate.txt`). Here is a reference [link](https://github.com/huggingface/transformers/tree/master/examples/tensorflow/language-modeling) on how to finetune a language model\n","* Any other modeling techniques you feel appropriate\n","\n","#### **Exercise Requirements:**\n","* At a minimum beat the public leaderboard score of **0.64** (benchmark submission)\n","\n","\n","\n","<br>\n","\n","**Remember to upload your submission files to Kaggle and also submit your notebook to Canvas at the end.**\n","\n","<br>\n","\n","**<font color=\"#f03b20\">The leaderboard for this competition will be computed based on `hidden` test set.</font>**"]},{"cell_type":"markdown","metadata":{"id":"FgY9xWhgGdt8"},"source":["## **<font color=\"#A41034\">Setup Notebook</font>**"]},{"cell_type":"markdown","metadata":{"id":"02ImzD1Kr85R"},"source":["**Installs**"]},{"cell_type":"code","metadata":{"id":"cj2LU7par9TN"},"source":["!pip install transformers datasets"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xsHQIdyQHAkV"},"source":["**Imports**"]},{"cell_type":"code","metadata":{"id":"dB7OG0AQAlha"},"source":["import os\n","import requests\n","import zipfile\n","import tarfile\n","import shutil\n","import math\n","import json\n","import time\n","import sys\n","import cv2\n","import string\n","import re\n","import subprocess\n","import hashlib\n","import numpy as np\n","import pandas as pd\n","from glob import glob\n","import collections\n","import unicodedata\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","import matplotlib.cm as cm\n","%matplotlib inline\n","\n","# Tensorflow\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras.models import Model, Sequential\n","from tensorflow.python.keras import backend as K\n","from tensorflow.python.keras.utils.layer_utils import count_params\n","\n","# sklearn\n","from sklearn.model_selection import train_test_split"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HwwaaoEAMmLg"},"source":["**Verify Setup**"]},{"cell_type":"markdown","metadata":{"id":"wD106cXQMm_8"},"source":["It is a good practice to verify what version of TensorFlow & Keras you are using. Also verify if GPU is enabled and what GPU you have. Run the following cells to check the version of TensorFlow\n","\n","References:\n","- [Eager Execution](https://www.tensorflow.org/guide/eager)\n","- [Data Performance](https://www.tensorflow.org/guide/data_performance)"]},{"cell_type":"code","metadata":{"id":"gHjjqJjIMtFH"},"source":["# Enable/Disable Eager Execution\n","# Reference: https://www.tensorflow.org/guide/eager\n","# TensorFlow's eager execution is an imperative programming environment that evaluates operations immediately, \n","# without building graphs\n","\n","#tf.compat.v1.disable_eager_execution()\n","#tf.compat.v1.enable_eager_execution()\n","\n","print(\"tensorflow version\", tf.__version__)\n","print(\"keras version\", tf.keras.__version__)\n","print(\"Eager Execution Enabled:\", tf.executing_eagerly())\n","\n","# Get the number of replicas \n","strategy = tf.distribute.MirroredStrategy()\n","print(\"Number of replicas:\", strategy.num_replicas_in_sync)\n","\n","devices = tf.config.experimental.get_visible_devices()\n","print(\"Devices:\", devices)\n","print(tf.config.experimental.list_logical_devices('GPU'))\n","\n","print(\"GPU Available: \", tf.config.list_physical_devices('GPU'))\n","print(\"All Physical Devices\", tf.config.list_physical_devices())\n","\n","# Better performance with the tf.data API\n","# Reference: https://www.tensorflow.org/guide/data_performance\n","AUTOTUNE = tf.data.experimental.AUTOTUNE"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yBRyDL1GMwj0"},"source":["Run this cell to see what GPU you have. If you get a P100 or T4 GPU that's great. If it's K80, it will still work but it will be slow. Make sure you start this exercise early, as training might take time."]},{"cell_type":"code","metadata":{"id":"DbysV9VCMxDy"},"source":["!nvidia-smi"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6i3sZbohM2K_"},"source":["**Utils**"]},{"cell_type":"markdown","metadata":{"id":"AIn5czLvM2sS"},"source":["Here are some util functions that we will be using for this notebook"]},{"cell_type":"code","metadata":{"id":"wm_puO9WSoq3"},"source":["def download_file(packet_url, base_path=\"\", extract=False, headers=None):\n","  if base_path != \"\":\n","    if not os.path.exists(base_path):\n","      os.mkdir(base_path)\n","  packet_file = os.path.basename(packet_url)\n","  with requests.get(packet_url, stream=True, headers=headers) as r:\n","      r.raise_for_status()\n","      with open(os.path.join(base_path,packet_file), 'wb') as f:\n","          for chunk in r.iter_content(chunk_size=8192):\n","              f.write(chunk)\n","  \n","  if extract:\n","    if packet_file.endswith(\".zip\"):\n","      with zipfile.ZipFile(os.path.join(base_path,packet_file)) as zfile:\n","        zfile.extractall(base_path)\n","    else:\n","      packet_name = packet_file.split('.')[0]\n","      with tarfile.open(os.path.join(base_path,packet_file)) as tfile:\n","        tfile.extractall(base_path)\n","\n","class JsonEncoder(json.JSONEncoder):\n","  def default(self, obj):\n","    if isinstance(obj, np.integer):\n","        return int(obj)\n","    elif isinstance(obj, np.floating):\n","        return float(obj)\n","    elif isinstance(obj, decimal.Decimal):\n","        return float(obj)\n","    elif isinstance(obj, np.ndarray):\n","        return obj.tolist()\n","    else:\n","        return super(JsonEncoder, self).default(obj)\n","\n","experiment_name = \"models\"\n","if not os.path.exists(experiment_name):\n","  os.mkdir(experiment_name)\n","\n","def save_data_details(data_details):\n","  with open(os.path.join(experiment_name,\"data_details.json\"), \"w\") as json_file:\n","    json_file.write(json.dumps(data_details,cls=JsonEncoder))\n","\n","def save_model(model,model_name=\"model01\"):\n","\n","  if isinstance(model,TFRobertaForSequenceClassification):\n","    model.save_weights(os.path.join(experiment_name,model_name+\".h5\"))\n","  else:\n","    # Save the enitire model (structure + weights)\n","    model.save(os.path.join(experiment_name,model_name+\".hdf5\"))\n","\n","    # Save only the weights\n","    model.save_weights(os.path.join(experiment_name,model_name+\".h5\"))\n","\n","    # Save the structure only\n","    model_json = model.to_json()\n","    with open(os.path.join(experiment_name,model_name+\".json\"), \"w\") as json_file:\n","        json_file.write(model_json)\n","\n","def get_model_size(model_name=\"model01\"):\n","  model_size = os.stat(os.path.join(experiment_name,model_name+\".h5\")).st_size\n","  return model_size\n","\n","def evaluate_save_model(model,test_data, training_results,execution_time, learning_rate, batch_size, epochs, optimizer,save=True):\n","    \n","  # Get the model train history\n","  model_train_history = training_results.history\n","  # Get the number of epochs the training was run for\n","  num_epochs = len(model_train_history[\"loss\"])\n","\n","  # Plot training results\n","  fig = plt.figure(figsize=(15,5))\n","  axs = fig.add_subplot(1,2,1)\n","  axs.set_title('Loss')\n","  # Plot all metrics\n","  for metric in [\"loss\",\"val_loss\"]:\n","      axs.plot(np.arange(0, num_epochs), model_train_history[metric], label=metric)\n","  axs.legend()\n","  \n","  axs = fig.add_subplot(1,2,2)\n","  axs.set_title('Accuracy')\n","  # Plot all metrics\n","  for metric in [\"accuracy\",\"val_accuracy\"]:\n","      axs.plot(np.arange(0, num_epochs), model_train_history[metric], label=metric)\n","  axs.legend()\n","\n","  plt.show()\n","  \n","  # Evaluate on test data\n","  evaluation_results = model.evaluate(test_data)\n","  print(evaluation_results)\n","  \n","  if save:\n","    # Save model\n","    save_model(model, model_name=model.name)\n","    model_size = get_model_size(model_name=model.name)\n","\n","    # Save model history\n","    with open(os.path.join(experiment_name,model.name+\"_train_history.json\"), \"w\") as json_file:\n","        json_file.write(json.dumps(model_train_history,cls=JsonEncoder))\n","\n","    trainable_parameters = count_params(model.trainable_weights)\n","    non_trainable_parameters = count_params(model.non_trainable_weights)\n","\n","    # Save model metrics\n","    metrics ={\n","        \"trainable_parameters\":trainable_parameters,\n","        \"execution_time\":execution_time,\n","        \"loss\":evaluation_results[0],\n","        \"accuracy\":evaluation_results[1],\n","        \"model_size\":model_size,\n","        \"learning_rate\":learning_rate,\n","        \"batch_size\":batch_size,\n","        \"epochs\":epochs,\n","        \"name\": model.name,\n","        \"optimizer\":type(optimizer).__name__\n","    }\n","    with open(os.path.join(experiment_name,model.name+\"_model_metrics.json\"), \"w\") as json_file:\n","        json_file.write(json.dumps(metrics,cls=JsonEncoder))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"d1BzqnfOVwuk"},"source":["## **<font color=\"#A41034\">Dataset</font>**"]},{"cell_type":"markdown","metadata":{"id":"o9hohypOVzX2"},"source":["#### **Download**"]},{"cell_type":"code","metadata":{"id":"ZZP5r9sAVzvx"},"source":["start_time = time.time()\n","# Dowload the dataset\n","download_file(\"https://github.com/dlops-io/datasets/releases/download/v1.0/arxiv_astronmy_competition.zip\", \n","              base_path=\"datasets\", extract=True)\n","execution_time = (time.time() - start_time)/60.0\n","print(\"Download execution time (mins)\",execution_time)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3bhtRhKfV_8k"},"source":["#### **Load & EDA**"]},{"cell_type":"code","metadata":{"id":"1bxevi-PWB_U"},"source":["# Your Code Here"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_AFAkCSbWGpk"},"source":["## **<font color=\"#A41034\">Build Data Pipelines</font>**"]},{"cell_type":"code","metadata":{"id":"HoSaYFL7WVeS"},"source":["# Your Code Here"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3eJGLU8MWQ_k"},"source":["## **<font color=\"#A41034\">Build Text Classification Models</font>**"]},{"cell_type":"code","metadata":{"id":"dRHjRhgKWU0u"},"source":["# Your Code Here"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZYd-weIIs4H_"},"source":["## **<font color=\"#A41034\">Submit to Kaggle</font>**\n","\n","* Make predictions on test datasets\n","* Prepare `submission.csv` file\n","* Upload using the [Kaggle](https://www.kaggle.com/c/ac215-fall-2021/submit)"]},{"cell_type":"code","metadata":{"id":"azWL0GUItPZN"},"source":["# Your Code Here"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sOFldshPJ_k5"},"source":["## **<font color=\"#A41034\">Kaggle Team Name</font>**"]},{"cell_type":"markdown","metadata":{"id":"6YJmj9w1KGRv"},"source":["Please provide your **Team Name** to Kaggle that you used to make your submissions. "]}]}